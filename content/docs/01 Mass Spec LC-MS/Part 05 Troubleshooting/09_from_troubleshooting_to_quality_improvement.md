---
weight: 9
title: "Translating From Troubleshooting to Quality Improvement: LC-MS/MS Case Histories"
authors: William Clarke, edited by Lenny Lin
categories: null
tags: null
description:  
draft: false
date: "2022-10-01"
lastmod: "2022-10-01"
series: null
toc: true
---



<!--more-->
---

It's a great pleasure to be able to introduce today's speaker, Dr. William Clark, who's associate professor at the Johns Hopkins University School of Medicine. And he's also a clinical chemist and the director of toxicology at the Johns Hopkins Hospital. I just want to before Bill start to presentation, put today's webinar in context with the previous three. The first was we just talked about basic instrument problems. And if you like the generic approach to troubleshooting next week, Dr. Pepe and the following week, Dr. Rockwood really talks about application specific problems related to pain management, monitoring, and to endocrinology, let's say. And today, the court kind of has a mix up of all kinds of cases, which I think is great, but he's also looking at it from a perspective that I think is important, which is what is it that you can learn from your troubleshooting problem, and then change your process or change your method, whatever is appropriate. So it doesn't happen again. And I think that is that really completes the circle when you're doing troubleshooting. And it's an important aspect. So without further ado, Bill, it's all yours.  

Okay, thanks, Judy. Yeah, so, so my cases are a little bit different, and they're not as focused or specialized. I'm very grateful to the folks in my lab, for sharing some of these things with me so that I can, I can put them together and slides for you. So I'm also grateful that I was able to make it into the office today. And you don't have to, in addition to hearing my voice, hear my kids running around and the dog barking, so everybody wins with me in the office.  

## Slide
Before I get started, I do need to give the following disclosures for the grant and research support as well as the my consulting activities, the most pertinent ones, of course, being both grant research and consultant relationships with Thermo Fisher and Shimadzu, since we're talking about mass spectrometry today. 

## Slide
So with that, we'll jump right into the cases. So the first case we have is a the case of a drug called Voriconazole. So this is something we use in our university reference lab for therapeutic drug monitoring. And this is used in a couple of cases, to treat things like invasive aspergillosis. Often times, it's used for prophylaxis and immunocompromised patients. And so it's very, as with most of the lab developed tests we have, it's very important that we not have to send this out to the local reference laboratory, we need to get these results as quickly as possible. And so this is one of the early tests that we brought up in our laboratory. And in this scenario, it's a test that's been running fine for weeks. In our, in our laboratory, it's it's the concentrations are not very low, it's typically measured in micrograms per milliliter. So it's not a technically challenging assay. But in this case, we see <u>shifted retention times</u> and <u>decreased peak intensity</u>. 

## Slide
So you'll see this example peak here, you'll see a peak area of roughly 150,000. So this is where our peaks are typically for this particular quality, QC peak. And so that's the standard one. 

## Slide
And then and in the follow up case, what we'll see the problem chromatogram. Now, I know it seems like it wouldn't be a problem to have a much higher peak area. In this case, the peak area is closer to 2 million. But you'll also notice that the retention time is shift by shifted by almost half a minute, I think it's if you if you look at the retention times it goes from 1.73 to 2.17, so about 0.4 minutes, and so we now have a later eluding peak. And this is something that we we track on a regular basis. And as one of our quality control measures. And so now we've seen this peak shift, and we've seen a great increase in the peak area which was unexpected. And so this triggered us to start start our troubleshooting. 

## Slide
And this actually sets us to the the first polling question which is for those of you doing clinical mass spectrometry, do you track retention time for the analyte of interest as a QA measure if you would just this is a simple one yes for yes and no for no I'll give you a few seconds to respond.  

Okay, looks like most people have have I've responded what we have. I think what Chris said there were technical incompatibilities, what he really means is I'm using an Apple computer and that that is what's causing the problem. But what we have here on the feedback is we had 49 People answer, yes, they <u>track retention time for a QA measure</u>, and eight, as no. So it seems that the vast majority of you track retention time is as a QA measure, which we think is a good idea. 

## Slide
Before we move on further case, I wanted to change it a little bit, which would be so what do you think is the cause of the shifted retention time? And? And in this one, we would answer yes. For a or aging column? No, for B, which is the wrong type of column? Answer too fast. For a problem with sample preparation, or C and answer too slow. For a system link, or D. What do you think is most likely?  

Okay, feels it feels like some people are changing their mind. But we've got answers still coming in.
Okay. All right, it looks like it's stabilizing. So we have about 20 people feeling that it's an aging column. 10 people, that it's the wrong type of column. Six, people thinking it's a problem with sample preparation, not uncommon, and 13, thinking that perhaps it's a system leak. Now, of course, all of these things can impact retention time. So we'll give you some sense. So the majority of you think that it's an aging column, and then the next being wrong type or a system leak. 

## Slide
So we take a similar approach. So many of those things that you voted on are things that that we would think about. So as we approach this, our standard practice for this is, one of the first things we do is <u>check the LC system</u>, it's, it's oftentimes the weaker part of the system in general. And so we would start by checking for leaks, and then also looking at <u>the pressure traces</u>. So we would access the systems data and ensure that we would compare the existing run pressure trace to the historic or average pressure traces. And then we start, if we don't see any obvious leaks, then we move into replacing the disposables and making fresh reagents. So in this case, we we checked for leaks, we didn't find any, we replaced the column, it did not fix it. Then we also made fresh preparation of the protein precipitation, reagent and internal standard. And after remaking all these things in an iterative fashion, the problem was still not solved. 

## Slide
So to give you some sense of what the method looks like here, this is a simple, we use a turbulent flow chromatography methods. So we'll do a protein precipitation followed by online pretreatment. And then we would have, it's a simple five minute method with a two minute ramp from 60 to 100%, B. And mobile phase A is water with 0.1%, formic acid and mobile phase B is the acetonitrile with 0.1% formic acid, and then it's a standard C18 column. 

## Slide
For mass spectrometry, this is selected reaction monitoring, looking at the transition of Voriconazole, from 350 to 281. And we do have an isotopically labeled internal standard. So looking at both of these things, there's nothing exotic about the method. I'm sure it's very similar to methods that you all have in your laboratories. 

## Polling Question #3
So that gets to you saw what the initial investigation yielded, we changed the columns, we changed the sample preparation, we made fresh reagent, what additional factors should be considered for the peak shift? And we here we would think about solvent mixing? mobile phase composition? could be a hardware problem, pump malfunction, or could we have programmed the wrong chromatographic method. We'll wait a few seconds for for those of you to respond.  
Okay, those silences sure seem longer than they are, of course. But anyway, so looking at the responses and those of you responding, thanks, we've got about, I'd say, a 50% response rate here. And if we look at the the answer to this, you're not going to many people think of it as solvent mixing or using the wrong method. And you're about equally split, we have 15 for the mobile phase composition and 16 for pump malfunctions. And so certainly, the solvent delivery would would impact the retention time. 

## Slide
And so when we look at this, those of you who answered mobile phase composition, you are correct, you are the You are the greatest detectives on the call, at least for this problem. So what we found is that the standard operating procedure was not followed, you know, we put the SOPs in the laboratory for a reason. But sometimes people get very comfortable. And they try to do this from memory. And so when we looked at this, we found that the the technologist in question, actually started using methanol instead of acetonitrile. And it was used for mobile phase B for several weeks, causing an erroneous increase in signal and the slight retention time shift, when we were able to go back to using the SOP actually reading it, the signal drops. And so there are two interesting factors of this. Number one, is that even though I presented the case, as the the the the increased peak area was the was the precipitating factor, it was indirectly the precipitating factor for for investigation. Actually, the mindset was, well, the more signal you have, the better. And so this didn't trigger, remember, we have several layers. You started with me at the top, but I'm not allowed to touch the instruments. And then you have our lab manager. And then you have somebody who's in overseeing the clinical operations. And then we have our bench technologists. And so the bench technologists did not trigger an investigation or report a problem until they saw that signal dropped. And actually, the lower signal was what was appropriate. So when we tracked back through this, the lower signal and early retention time actually was what was appropriate. The other thing I want to point out is the that it took weeks, for us to find this. likely it was okay. Because we run our quality control samples, and we do this but we didn't have any sample for reinjection at this point. So honestly, that's the that's what keeps somebody up at night, right, which is that you've made some sort of mistake, you've released erroneous results, and you can't get those back and you can't communicate those. And so it's what we found is that the the records for QA QC must be in terms of, you know, we track retention time, we track slope of the calibration curve, and we track these peak areas, and we keep them as a running average, as well as listing each of the individual day. And those have to be reviewed by a member of the supervisory staff every day that we run in order to catch any of these trends. So it's a very manual solution, but it's what we think is really important to be able to to be able to catch any of these areas early. 

## Polling Question #4
[13:45]Okay, so we're going to shift gears here to another case, and this is really in the area of our drugs abuse screening for pain management. And before we get into the specifics of the case, I wanted to ask whether you believe, you know that there's the chance of whether we use high resolution mass spectrometry using a time of flight or orbitrap type instrument or we can use nominal mass resolution or little better in using linear traps or triple quadrupole. So, the question is, is high resolution accurate mass not impacted by structural isomers or isobaric compounds is this true or false?  
Okay, so we'll walk you through a little bit in a case what I would say is about in a three to one ratio, most people responded that this is false that high resolution accurate mass methods can be impacted by isobaric, or structural isomer isomeric compounds and we want to walk you. I agree with that, that it is false. And we'll walk you through this case example and show you how we, I had hoped that that statement was true, but it was not. 

## Pain management
So for our pain management, we actually use a high resolution mass spectrometer, but we use it as a screening method. So in this case, we would consider it to be screening or presumptive, because we do not use an internal standard for each. each compound of interest we have in the menu contains about 45 to 50 different compounds all in the positive ion mode. And we but we have a single process control, at least as the as the method was initially designed. So how do we call a positive screen versus a negative screen, and we really do it on qualitative factors. The first one is that it would have to be in the correct retention, what I say here, retention time, but it's actually a window, right? Because the tighter that window is, as we saw previously, and we'll see in a subsequent case, the retention windows, if it's too tight can can be problematic as your system ages, both from the reagent and the column perspective. So it must be in the correct retention window. But then it also would have to have the correct accurate mass. So in this current system, it's not, we don't have the ability to do selective reaction monitoring. It's a standard orbitrap mass spectrometer, but ours is one that has the capability to do high collision energy fragmentation. And so we're able to see the accurate mass of the precursor ion or parent compound. Our criteria were within five parts per million. And then, so we had to be in the retention window, we had to have the appropriate mass, accurate mass within the tolerance listed. And then we'd have to have the presence of at least one fragment. And so if those three things were there, then we can make this a positive call. But it is a screening assay. And so we wouldn't consider this to be definitive or confirmatory testing, we just, we built it this way, where we felt we could do rapid screening with less false positives. And we thought that it would be easier. It did not turn out to be so. 

## Problem?
[17:38] So what was the problem that we ran into? Well, our acceptable retention window is relatively wide, 0.4 minutes. And, and this has to do with the samples themselves are a little bit dirty. And, and so we, and we use this over several columns. And oftentimes we're having to do, there's a relatively high repeat rate for this. And so the compounds would shift. And so what we found very early on is that we'd had five pairs of structural isomers or isobaric, compounds that elute closely together, our assumption was that even if they're, they have the same mass, we will be able to differentiate them on the basis of retention time. But we ran into a problem on the first proficiency testing event after we went live with the method. The interpretation was difficult. And it resulted in an event failure. So we we failed our PT testing. 

## Sample Chromatogram: Methamphetamine with Morphine d3 IS
And here, you'll see why. If you look at the peaks in frame one, and frame three, you have methamphetamine and phentolamine. But those are not two individual sample components. If you look at the intensity, you'll find that it's identical, approximately two and a half million for both frame one and frame three. And it's an identical retention time. And this is a function of the software. So the software looks at these and says, Hey, you're in the appropriate retention window, which is relatively wide, you're sitting at the same mass. So therefore, I'm going to identify you in both. So if you look at this, you would say, well, you'd say either it's a coin toss for whether it's methamphetamine or phentolamine. Or both compounds are present, but that's highly unlikely for both compounds to be present at exactly the same retention time at exactly the same intensity. That's that's one thing being identified twice. And you'll see in frame two that that's where the morphine internal standard peak is for process control. The other thing that I would point out is you'll see again, the detection criteria. So there's retention window criteria. Yeah, if you look at the delta ppm, it's 0.5. So it's, it's quite close to the projected, projected accurate mass. And then you'll look in the very last columns, you'll see the fragments, and we look for three fragments for each compound. So we get back to the problem, which is what are we supposed to do here? We don't. Clearly methamphetamine and feta mean are not the same thing. The patient is not likely or subject is not likely to have both on board. So how are we to differentiate this? 

## Method Details - Chromatography
[20:30] Well, the method itself is fairly straightforward. Again, it's in this case, it is a simple, what we would call ALEKS only method there is no online cleanup of the sample. The protein precipitation step is used with us. I believe methanol for the protein crash from the urine, we don't do a dilute and shoot we still do a protein crash even with urine. And then we apply it 100% A and then we have a multistage ramp over five minutes to 100%. B. But again, relatively standard chromatography. For mass spectrometry, there's not a lot to troubleshoot in that it's a full scan, and with all ion fragmentation. So if you do mass spectrometry based screening, do you perform confirmatory testing for positive screening results? And so you would answer yes or no. And if you don't do any mass spectrometry based screening, you could answer too fast if you don't do mass spec base screening.
Okay, so of those that have answered, we have about 30 that say they don't perform mass spectrometry based screening of the 20. That answered, they do perform mass spectrometry based screening, the vast majority of them do perform confirmatory testing, with only four stating that they do not perform confirmatory testing, I would say in our laboratory, we don't perform the confirmatory testing, we would actually send it to a different laboratory to do standard confirmatory testing there. Okay, so what was our solution for this, what we determined to do was to, for each isobar isomer, we include an isotopically labeled internal standard. And so in the case where we would have to a component that's in two different windows, we have an indeterminate result, the internal standard we use as a tiebreaker, based on the retention time and for identification of the compound and, and here you'll see are the isobars that we that we run into just within our pain management panel, there are five sets of them codeine and hydrocodone, methamphetamine and phentermine, which we showed earlier, the morphine and hydromorphone, more phone, oxymorphone, and nor oxycodone, and then Naloxone and six months acetyl morphine. And in each case, you'll see that these are very important to be able to differentiate one from another in the context of pain management. So we really have to solve this problem relatively quickly. Now, what you'll see in this, that now we have four frames here, and so we were using the same methamphetamine phentolamine. Example, again, in frames one and three, you'll see an identical peak, right? So the retention time was 2.69. And you'll see they have the exact same identity, and I'm sorry, not identity intensity, little difficulty reading there. And they have about two and a half million peak intensity. But what you'll see in frame two, or peak two is that's the deuterated methamphetamine, and in the last panel, or panel four, you'll see the deuterated center mean, all right, and you'll see that the methamphetamine internal standard elutes at 2.69, whereas the phentermine internal standard elutes at 2.96 minutes, so even though they're within the same retention window right there, they're our retention window is point four minutes, and they're eluding within point two minutes of one another. With these internal standards, we're able to use that as a tiebreaker. And from a screening perspective, state that this will be positive for methamphetamine and not positive for phentermine. Again, if somebody were to ask us, we would still send that out for confirmatory testing, if there was still a question clinically about that result. Okay. So we're gonna shift gears a little bit to another screening approach. For this is a case contributed by our lab manager called Chi, she likes to name things a little differently. So this is the case of the Miss missing emtricitabine. And so in this particular method, this was a high throughput antiretroviral panel that was developed for the HIV prevention trials network. And so we use this in in the context of pre exposure prophylaxis trials or treatment as prevention trials. And it's used to determine adherence. So are, you know, to interpret the results properly, we need to make sure that people are taking their drugs to prevent the spread of the virus, or we need to make sure they're not taking doing off study drug use. So perhaps study participants that are sharing drugs with one another if it's if it's a partner based study, or making sure that they're not enrolled in multiple trials. And I have a whole nother presentation on this topic. But the short version is that this is a real problem in many studies. And so for appropriate data interpretation, we must have the ability to rapidly check for these screens. And so we have a 20 component anti retroviral panel, we can screen for these 20 compounds in about 90 seconds. And so this has to be a very tightly controlled method, we're switching from a previous method that was 100% qualitative, where it was still a little bit subjective. Similar to the previous example, we're now we're doing a data dependent acquisition where it's semi quantitative, so we have some key concentration markers, that were able to at least to get a ballpark measure of
the concentration of these compounds in the blood. So as we're going into the validation, we think we have the problem solved with the software we haven't made, we've locked down our method, we have no changes in solutions, columns, mobile phases, etc. And all of a sudden, the interest that had been goes missing, and this is an important compound, it's not something that would be rarely used, it should show up somewhat frequently. And more importantly, the rest of the assay looks pretty much the same. So we have 20 components, nine look just fine. And all of a sudden, emtricitabine just disappears. So if we look at the problem chromatogram. This chromatogram looks a little bit different than then we we often see. So the top would be the total ion chromatogram. So this includes all 20 components, all 20 internal standards. And so it is a little bit messy, and there's not a lot you can find from it. In panel two, this would be the this is now done on a q exactive. With data dependent acquisition. And so, you know, in a in a two minute method, with 92nd screen time, you know, we don't get a lot of chromatographic revolution there. So this peak is done on the quadrupole, it's a little bit messy, it's nominal mass resolution. And so in the absence of any fragments, it doesn't really tell us a lot. And so the important chromatograms in terms of determining positivity are the next five panels, which you can see are all blank. Right. So this means that we don't have any emtricitabine in this sample, which is a problem because we know we put it there. These are not patient samples, these are samples that we spiked ourselves, and so we either spiked it with water, or we have a problem with our method. So on investigation, this was discovered during stress testing of the assay. And so the column has about 500 injections, we don't have any loss and signal for any other components of the assay. And it's very tightly programmed. The the 20 components are alluded in two minutes, as I said, and there are stringent time segments. For each ARB to optimize the duty cycle we need, we want to have more scans per peak, because we don't, again, sparing you the long story in various iterations of this we have not we've had sub optimal number of scans per peak, and we can get indeterminate results. And this is what we do not want to do because when we're testing 8000 specimens, for a clinical study, we don't want to have to go back and manually review each chromatogram you know, with three scans per peak, and then have to make a subjective judgment as to whether that those three scans actually is a good peak or not. So we want to take a look at we want to take a look at this and figure out the problem. So polling question number six. What do you think is the cause for this disappearing sample component? Is that math calibration, which would be yes. A leak in the system, which would be no reagent stability, which you would answer too fast or column degradation which would be too slow
Okay, so looks like we've got a majority of the answers that are in, we have a couple of folks that think perhaps it's a leak in the system. We have equal numbers, the two most popular answers equal numbers of folks that think it's mass calibration or reagent stability. And then we have a smaller amount about eight out of 50. That suggest, let's sorry, quick math in my head about 20%. Right? About 20% think that it is column degradation. Well, as we look at this, we actually found that it's the column age, the emtricitabine, peak window fell just outside of the acquisition window, the original window for data acquisition was point six to one minute, so 0.4 minutes and, and we use those as standard Windows. But what we found is that when you were able to widen out those windows or get rid of the windows altogether, you'd find that over time, those those peaks shifted as we're doing the stress testing. And so what we've done then, because we have the high resolution fragments, and multiple fragments, at that as our safeguard, we felt comfortable relaxing the data acquisition window, and as we put in the new window of 25, to one minutes, over the course of several 100 injections, we found that we did not lose our emtricitabine, which is, which is good for us. And so here, you'll see a data example, and this is what it is supposed to look like. So you'll have to ignore the total ion chromatogram, for the most part, as this is just one component of a much larger mixture. But again, I'd focus your attention on the bottom five panels, and you'll see five relatively well described peaks that are the different fragments of interest that are being in so we were able to get it back. So another one would be a case of diminishing signal. So in the board Canosa, we had an increasing signal. And this would be an example where we're losing signal during the course of the analysis. And so we, we have in our going back to our university reference lab, we have a drug that we monitor called hydroxychloroquine, which is also the trade name is Plaquenil. And this is a drug that has been used as an anti malarial and is but more recently, it's used to manage lupus flare ups. And so as a Hopkins rheumatologist that requested we set up this assay, and we we do a fair number. And again, this is another one of those assays that is not technically challenging in terms of limit of quantification and assay sensitivity. This is it's a robust, it's a whole blood assay. So, you know, it is possible that, you know, after a protein precipitation using whole blood, we could get a dirty source. And we run, we actually run this just once a week, the clinic meets twice a week on Thursdays and Mondays. So we're able to actually run the assay on Tuesdays and in get the results back in a timely fashion for this outpatient clinic. And so, again, we you know, as the VOR Connellsville assay is a much older assay in the laboratory. So we learned from lesson learned some lessons from that. And we actually now catch changes in peak air and retention time a little bit more quickly. And so what we find here is the person reviewing the records saw an eternal standard peak area of about 35,000, whereas the average peak area that we track is 500,000. So we've lost by an order of magnitude. And this is concerning and nothing we can do could bring that back. So the question for you is what do you think is the factor that you think is the cause for a loss of signal intensity? It could it be mass calibration? Answer Yes. If it's a Do you think it's sample preparation? Answer? No. If you think it is the aging column, answer too fast. If it's reagent preparation, answer too slow. And if it's system leak, answer with applause, which is just an obvious effort for me to get some applause during the presentation.
Okay, so we have most of the 50 that have been responding have weighed in, we have about, see an even split of 30 of you 15 are suggesting its sample preparation. The other 15 are suggesting reagent preparation. Small number, say agent cat column, a small number suggests mass calibration. And so Oh, sorry, yeah, I'm right. And then so people think it's for file system leak, or sample preparation, small amounts and reagent preparation, and an even smaller number are laughing at me. All right. So we don't have a, we don't have a majority opinion. So let's take a look at what we found. We jumped in to look at the pressure trace. But we didn't find any obvious leaks, we didn't see any liquid coming from any of the connections, the pressure traces are fine. So the chromatography system, we rolled it out pretty early on, when we did an infusion of standards. Looking at the various parameters, we found that the mass spectrometer was functioning as it should. Reviewing our records, we found no change in column mobile phase, internal standard or extraction solution. But when we went to inject previously analyzed, analyzed samples, we found the same low signal. And so we replaced a column, we made new mobile phases, we did a new extraction of the older samples. And none of these things corrected those signal. So what I would tell you is that of all those five answers, that it was a little unfair, and that I did not include the correct answer, all of you were incorrect. All right, which would have been incorrect to this, what we found is that, during weekly cleaning, the capillary temperature was decreased to 100 degrees from its setpoint of 200 degrees to make changing of the capillary easier, this was not how we train changing of the capillary. And then Compounding this change was that the assay tune method was saved at this lower value. And so when it was restored to the original value of 200, the signal immediately went back to where it should be. And this is, this is presented in about five minutes. This really took us three or four weeks of much hand wringing and intense discussion of what could be going on with our instrument only to figure that this is some small thing. And so the idea here at the left is don't take for granted that things are the way you left them, we would say that, if you look at all these things, the cases were presenting about human error accounts for about 50% of the assay failures, particularly when it's newer staff staff, less than one year. And so the important thing is to have these system checks in place, and multiple quality assurance measures, but also somebody coming behind, it's not enough to write down the number, it's really important to have somebody come in behind and make sure that that situation is actually being monitored. And we're looking for trends and other things that that can that can impact that. And so once we change the capillary temperature signal is restoring you see this internal standard peak area of about 580,000, which was slightly above our average internal standard peak area. So we have one last case to share with you before the q&a and but the question I want to ask you is this is another yes, no. So we're places where you have to do manual transcription. Do you have checks in place for transcription errors?
Okay, I'd say most of you have weighed in and almost exactly to the one. We have to do one Bing, yes. Which is terrific. A lot of times we sort of, especially in smaller volume labs, we take it for granted that if as long as we don't have time pressure, that the transcription won't be a problem because people will take their time and be careful, but that's not not always the case. So we want to look at this. This is a case that we presented last summer at ACC and it it's actually related to the high school High Throughput antiretroviral screening. So our screening method utilizes robotic sample preparation, we use something like a T cam, but we know that there are other automated robotic solutions out there. And then we use for channel channel chromatography, and then the high resolution mass spectrometry. We're fortunate, at least in that, so our robot has barcode reading capability, and the samples are bar coded. But on large scale analyses, you know, greater than 5000 specimens, transit, transcription errors can still occur. And the question is, how can we minimize or eliminate this problem? And we'll talk a little bit just briefly about where these transcription errors come from, even with barcode and samples, right. So to do that, we have to think where do the transcription errors occur. And so if we, this was the list that I just sort of put together, which was, we're going to have happen, what can happen at collection, right we in the clinical laboratory, we see this more than we want to, which is people mislabeled a specimen there. Or perhaps the specimen is labeled correctly, but a transcription error is made on the specimen manifest. And these are things that happen outside of the laboratory. But there are plenty of steps that can happen within the laboratory, including making the aliquot from the original sample, we use the robot for that, the specimen processing, we use the robot for that transfer to well plates or specimen vials built, you know, to put it on the mass spec, building of the LCMS load and then reporting of results, there are still many opportunities, even if you get pristinely labeled 100% accurate specimens from outside of the laboratory, which honestly never ever happens. So if we look at this, we just boxed in the the within lab errors. And specifically, if you look at these, we're going to talk about solutions for these three making specimen Alec Watts from the original sample, as long as it is. As long as the barcode printer is working, that usually is pretty easy in the transfer to weld plates or specimen vials, we actually just take the plates from the robot, and we put those plates on the auto sampler. But if you have a barcode failure, where specifically if the barcode is smudged or ripped or wet, that can cause problems. And then because the systems are not all interconnected, there are opportunities to to make a problem when we when we build the load. And then when we have to summarize the results. And so solution number one that we have is if there's a barcode failure, we need a failsafe. And in this case, our failsafe or our process to avoid the manual transcription errors is we have a two person system for any manual data entry. It's cumbersome. It's not particularly innovative or creative. It's just sort of brute force. How do we get away from this. So we keep a processing log. If there's manual data entry, it requires a processor and an observer. Both of them initial the logs, so we know who to go back to the manual data entries are noted on the log and and then we track it through that. So we at least know we have two sets of eyes doing this. Again, not an elegant solution, but it's certainly as effective. So between that system and and barcode reading robot, that usually does us pretty good in terms of getting from the initial samples to our clots, and then process and all of that's controlled in the robot. So we have our extracted samples in this plate, the now we have to move that plate onto the
onto the autosampler. And then we have to build our LCMS load. And we want to eliminate that manual data transfer. So our short term solution here was the format that he can output to match the LCMS input needed for our software. And then we cut and paste. This is an imperfect solution. And I put that there because we still have some problems you can you can generate what you would call a tail, a data tail, right? If you have an incompletely filled plate, and you're just working from a template, you may have your first sample with all 96 ones covered. And then your next plate has only 90 samples. But if you import that entire sheet, you've got six samples that now look like they've been analyzed twice, even though those six wells at the tail end were empty. Previously, again, learned this the hard way. We had samples in our our big sample set that looked like they had been analyzed six and eight times when they hadn't really only been analyzed once and of course because those wells were empty. The multiple the multiple replicates did not match, because they were noise. So our longtime One solution is actually to get away from the cut and paste and use programming to transfer the T combat directly to the LCMS software. This requires vendor support both from the LCMS vendor and the T can or robotic vendor. That's, I'd love to tell you we've solved that we're still that's still a work in progress. And then last is for single batch analysis reporting, that's pretty easy, right? You can just have the output from your plate, you download it into a spreadsheet or paste it over, and you're really good. But when you're combining multiple batches over several days, this can be disastrous, especially if you have 20 plus analytes. So for us we did we had to do some custom programming. I'm fortunate that we have a young pathologist who knows how to do all that stuff, because I certainly don't. And so we utilize this custom programming just within a database to combine and sort multiple CSV outputs from the LCMS software. And then we're able to build those into a summary data report. And the nice thing is, then you can write in rules for custom data QC, you can you can see if something looks like it's duplicate specimens, or in our case, are we getting results that are weird regimens, that we may need to go back and manually review that data and look for carryover or splashes or something of that nature. And so that, that gives us a nice solution. But in the long term, and I know some of the newest software packages can assist in some way with this, you really have to have this be native to the vendor. Not everybody can, can do what Matthew can do. And so from that perspective, it really requires support from the vendors to do this. But you have to think really carefully about how you compile all those results from very large, very large runs. Okay, so that's all the cases that I have for today. I do want to acknowledge some folks autumn burrow in particular, she is the JG lab manager and honestly is our front, our frontline troubleshooter on these but also thank all the lab staff. Because they all do such a great job SAP at the Molly Aparna branch in a bad. And then, as I mentioned with the programming, Matthew Olson on the pathology faculty who has helped us with think through some of our problems, but also has come up with some of the software solutions that we use in our large screening efforts. And with that, I'm happy to take any questions at this time.
So thanks, Bill, that was awesome. I really appreciate your sharing all of those problems that all of us encounter, but we're not necessarily always willing to admit to. That's great. We do have one question that someone submitted during the presentation, I think it was in relationship to the first case and they said, How did you handle patient results? I'm not sure if they've met patient samples waiting to be tested or patient results that might need correction while you were doing the troubleshooting. I think this is something that's a challenge for all of us.
Yeah. So I think that there, there are two answers to the question. The easy one is once the problem was discovered, we just held the patients and you know, we'll usually hold them for 24 to 48 hours, it's not ideal. And we are in constant communication with the ID pharmacists in this so that we can. So we can judge the acuity of the situation. But if we notice a problem, we actually will put things on hold and if need be, we will expedite sending those to a reference lab on our dime. The other way that question can be taken was the samples that were resulted with the incorrect mobile phase when methanol was substituted for a CD nine trial. Now in that case, all the samples that were treated during that time, so the calibrators controls, and patients were all run with the A CT nitrile. So they all would have had increased peak areas. And if we think about this, from the perspective of you know, all the peak areas would should increase by the by the same proportion. And so since we have a separate stock of QC material versus calibrator, we do have an external check of that. Now the ideal thing would be to take archived specimens and repeat them to ensure that they were within, you know, 15 or 20% of the initial result, and we just didn't have that ability. So given that our QC was in we, we did not go back and modify those, because then in my mind, at least, I think that's a validation problem and that you used a non validated method, and I don't know how to explain that to a clinician Who would then say, well, so the method didn't work? And I would say, Well, I, I can't say that definitively. I think it creates more questions. So that's how we handled it. I'm not saying that's the the best way to handle it. But at that time, that's how we handled it. What we actually do now, is it we do save those specimens frozen for a longer period of time. And we would go back and retest those. And if need be, if there was a significant difference, we would go back and notify the clinician. The truth is, is that that dose adjustment after a day has already been made. So it's really more of a mia culpa than anything else.
Yeah, I think that's a great answer. Bill, I certainly my experiences, then that there are just some situations where you don't have the patient sample. So I don't think it's a black and white answer, I think you have to go back look and see what the clinical effects was, in some cases, that the largest sample like this that I've been affected with, with TSH when I was at Kaiser, and we had to look back, I want to say over two and a half years that we changed the method through like 3 million results to try to pick out the people that a small percentage of people that might have been affected. So I think I would have done exactly what you did. I had a question for you on case number one, which is do you think you had a higher signal when the that mobile case is being made with nazionale versus Osita? nitrile? Do you know or could you speculate was that simply because you had better ionization with methanol? Or do you think it was because that was eluding later? It was with a higher percentage of organic solvent? And therefore once again, better ionization?
GDF? That's a great question. And I have to tell you, those are the things that we we want to chase down those chasetown those threads, that we still have, you know, carryover questions on a different method from probably four years ago with core hexene. So we have the sort of this little toolbox of things that that we want to collect data on and just don't have time, I think from a speculative standpoint, we encourage during method development to try both. See the nitrile and methanol is when we're doing our infusion studies. So I don't remember that data, I suspect, I mean, typically, we would want to pick the thing that the compound items ionizes better. And so I think it's probably related more to the chromatography but that's, that's a guess that I just I just don't know the answer.
Now, I hurry. I have a long life list like that, too. So sorry, I wasn't trying to answer a gotcha question. But, but I do think that it's just something for people to think about is almost always we think when there's a degree of change in the method, we're gonna have worse signals. So it's, I'd like it that this was an example where there was better signal, which is you say, I think it's often that makes it more difficult for people sometimes to recognize I've had a scenario where the pressure is lower. And so people keep writing the instrument, even though in fact, lower pressure is a bad thing, because it means maybe there's a leak. But when people are used to that the system shuts down with high pressure, then I definitely had the feedback. Well, there wasn't anything wrong because the pressure was low. Yeah, I think that that's, that's, again, it's just a lesson for us in terms of that we assume sometimes that we don't have to put in a question. Yeah. How can I put it put in a criteria for somebody to check, but maybe we do. I'm waiting for another questions come up in an interim, I had a question for you. We also had a similar problem with identifying in our case, we identified I think hydromorphone is morphine or the other way around, because they were detected in the same manner, and that we followed up with that with corrective action, but by doing a few things, one of which was making narrower acquisition windows and defining them more carefully, so as to try to prevent How can I put it reduce the risk that you might pick up the wrong analyte? Is that any order that you might recognize that something has changed? Do you guys have like a retention time and then a window around? It's acceptable? How do you try to deal with that issue that you can't say there's an absolute retention time, that's gonna be the same all the time, but we know what's an acceptable range?
Yeah. So so the reason we have the wide retention window there is it's you strike a balance right between. If you have the narrow retention window, then you're going to have more flags of Miss compounds because things will fall out side of the retention window, but meet the other criteria. And also when you have column to column variability, you may have to go back and reprogram every time you're putting on a new column and run some limited verification work and so the more narrow your window, you are reducing that chance Tonight is something that we considered, we really came up with the the internal standard solution for us so that we could maintain those original windows for method development. So we weren't creating a bunch of QA work for us on the, on the other end every time we switched reagents or, or did column check ins or things like that. So, but you're right, that there is another solution, which would be to narrow down the window.
Okay, great. I think we have time for one more question before we have to sign off and not
sorry. There's one in the chat window?
Yes. I saw that with a column a column of a district different stationary states asking with that would help with the separation of the isomeric compounds?
Oh, yeah. So so we have investigated the PFP and highlight columns. And it changing the change in the chromatography is one of the things that we could change. And we're actually still batting around around the idea of, even though it's not high resolution work, going back to the more standard nominal maths on a triple quadrupole. Just because we think that that'll be sufficient. And probably if we do at least some limited quantification, you know, with a qualifier and a qualifier ion, we probably have spent a lot less time looking at full scan chromatograms. So yeah, that's that's a potential solution. Again, it's one of those things that you know, there's multiple ways to solve the problem. And that is something that we that we have considered, we just haven't made that change.
Okay, but it was the last question, but I did want to, do you have any comments about the question about carryover? Is that something that you feel is more of a problem with your turbo flow system, then? Where you don't have online extraction?
I think that that is just certainly it's a concern. And it's a it's a compound specific question. Some things you really we really struggle with. carryover. chlorhexidine was the great example. We did that in a neonate study. And that's well known to be problematic with carryover in the carry the carry over there is even to the into the auto sampler, not just on the turbo flow column, we have found, for the most part, that it's just a matter of optimizing the washers for us. That's not something that's a great concern, but it is something that we have to account for during our message development workflow.
Okay, great, Bill. Thanks so much. I really appreciate your sharing these cases and and your solutions to them. For all of the attendees. I think Chris is going to be sending out a survey afterwards asking, we're trying to determine whether should we should have another series on troubleshooting. So let us know your views on that, and particularly if there's any types of cases that you'd like to hear. So thanks, everyone, for attending and bill for a great presentation. And that is the last in the series of webinars. Chris, did you have anything to add?
Just a couple of notes. Thank you both Judy. And Bill. That was a great presentation, as Judy says, concludes the current LC Ms. Ms. Troubleshooting series. But we have more in development with Judy and with other participants. If you're interested in presenting a webinar with Ms. ACL, just get in contact with me. And let me know what you're interested in presenting to the community. But for now, a break until the completion of MSA still 2060s in Palm Springs. And we hope to see you there. Like giddy Thank you, Bill. Thank you everyone. Have a great afternoon and we will talk to you soon.
Thanks